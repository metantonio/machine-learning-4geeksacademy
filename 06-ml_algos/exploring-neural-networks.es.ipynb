{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando KERAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡En este cuaderno vamos a cubrir paso a paso cómo modelar tu primera red neuronal!\n",
    "\n",
    "Vamos a utilizar el conjunto de datos de inicio de diabetes de los indios Pima. Este es un conjunto de datos de Machine Learning estándar del repositorio de Machine Learning de UCI. Describe los datos de los registros médicos de los pacientes de los indios Pima y si tuvieron un inicio de diabetes dentro de los cinco años.\n",
    "\n",
    "Como tal, es un problema de clasificación binaria (inicio de diabetes como 1 o no como 0). Todas las variables de entrada que describen a cada paciente son numéricas. Esto hace que sea fácil de usar directamente con redes neuronales que esperan valores numéricos de entrada y salida, y es ideal para nuestra primera red neuronal en Keras.\n",
    "\n",
    "**Entender los datos**\n",
    "\n",
    "Variables de Entrada (X):\n",
    "\n",
    "- Número de veces embarazada.\n",
    "\n",
    "- Concentración de glucosa plasmática a las 2 horas en una prueba de tolerancia oral a la glucosa.\n",
    "\n",
    "- Presión arterial diastólica (mm Hg).\n",
    "\n",
    "- Grosor del pliegue cutáneo del tríceps (mm).\n",
    "\n",
    "- Insulina sérica de 2 horas (mu U/ml).\n",
    "\n",
    "- Índice de masa corporal (peso en kg/(altura en m)^2).\n",
    "\n",
    "- Función de pedigrí de diabetes.\n",
    "\n",
    "- Años de edad.\n",
    "\n",
    "Variables de salida (y):\n",
    "\n",
    "Variable de clase (0 o 1)\n",
    "\n",
    "**Requisitos:**\n",
    "\n",
    "- Python 2 ó 3 instalado.\n",
    "\n",
    "- SciPy (incluido NumPy) instalado.\n",
    "\n",
    "- Keras y un backend (Theano o TensorFlow) instalado.\n",
    "\n",
    "**Paso 1: instalar bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (1.8.1)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.25.0,>=1.17.3 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from scipy) (1.23.1)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.47.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from tensorflow) (1.14.1)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from tensorflow) (21.3)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from tensorflow) (62.3.2)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from tensorflow) (4.2.0)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.9.1-py2.py3-none-any.whl (167 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.8/167.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.27.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.4/232.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.5.18.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.8.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.5/151.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=e841deca723b0399f0c0c675f59521ea1dd329e069dbdd7d5820a743fb46009e\n",
      "  Stored in directory: /home/gitpod/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: termcolor, tensorboard-plugin-wit, pyasn1, libclang, keras, flatbuffers, werkzeug, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, pyasn1-modules, protobuf, opt-einsum, oauthlib, keras-preprocessing, h5py, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.2.0 astunparse-1.6.3 cachetools-5.2.0 flatbuffers-1.12 gast-0.4.0 google-auth-2.9.1 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.47.0 h5py-3.7.0 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.4.1 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0 werkzeug-2.2.1\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scipy tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar Keras, deberá tener instalado el paquete TensorFlow.\n",
    "\n",
    "Una vez que TensorFlow esté instalado, solo importa Keras. Usaremos la biblioteca NumPy para cargar nuestro conjunto de datos y usaremos dos clases de la biblioteca Keras para definir nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2: importar bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-28 05:13:18.859773: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-28 05:13:18.859912: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Primera red neuronal con keras\n",
    "\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 3: Cargar los datos**\n",
    "\n",
    "Ahora podemos cargar el archivo como una matriz de números usando la función NumPy loadtxt()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el conjunto de datos\n",
    "dataset = loadtxt('../assets/pima-indians-diabetes.csv', delimiter=',')\n",
    "\n",
    "# Dividir en variables de entrada (X) y salida (y)\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 4: Definir el modelo KERAS**\n",
    "\n",
    "Los modelos en Keras se definen como una secuencia de capas.\n",
    "\n",
    "Creamos un modelo secuencial y agregamos capas una a la vez hasta que estemos satisfechos con nuestra arquitectura de red.\n",
    "\n",
    "Primero, asegúrate de que la capa de entrada tenga la cantidad correcta de entidades de entrada. Esto se puede especificar al crear la primera capa con el argumento input_shape y establecerlo en (8,) para presentar las 8 variables de entrada como un vector.\n",
    "\n",
    "¿Cómo sabemos el número de capas y sus tipos? A menudo, la mejor estructura de red se encuentra a través de un proceso de experimentación de prueba y error.\n",
    "\n",
    "En este ejemplo, utilizaremos una estructura de red completamente conectada con tres capas.\n",
    "\n",
    "Las capas totalmente conectadas se definen mediante la clase Dense. Podemos especificar el número de neuronas o nodos en la capa como primer argumento y especificar la función de activación usando el argumento de activación.\n",
    "\n",
    "Usaremos la función de activación de la unidad lineal rectificada denominada ReLU en las dos primeras capas y la función Sigmoid en la capa de salida. Usamos un sigmoide en la capa de salida para garantizar que la salida de nuestra red esté entre 0 y 1 y sea fácil de asignar a una probabilidad de clase 1 o ajustarse a una clasificación rígida de cualquier clase con un umbral predeterminado de 0,5.\n",
    "\n",
    "Para resumir:\n",
    "\n",
    "- El modelo espera filas de datos con 8 variables (el argumento input_shape=(8,).\n",
    "\n",
    "- La primera capa oculta tiene 12 nodos y utiliza la función de activación relu.\n",
    "\n",
    "- La segunda capa oculta tiene 8 nodos y utiliza la función de activación relu.\n",
    "\n",
    "- La capa de salida tiene un nodo y utiliza la función de activación sigmoidea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-28 05:34:23.313842: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-28 05:34:23.314107: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-28 05:34:23.314184: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (4geeksacade-machinelear-646p8lv2dr4): /proc/driver/nvidia/version does not exist\n",
      "2022-07-28 05:34:23.314906: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Definir el modelo de keras\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_shape=(8,), activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 5: Compilar el modelo KERAS**\n",
    "\n",
    "Ahora que el modelo está definido, podemos compilarlo. El backend elige automáticamente la mejor manera de representar la red para entrenar y hacer predicciones para ejecutar en su hardware, como CPU o GPU o incluso distribuido.\n",
    "\n",
    "Al compilar, debemos especificar algunas propiedades adicionales requeridas al entrenar la red. Recuerde entrenar una red significa encontrar el mejor conjunto de pesos para asignar entradas a salidas en nuestro conjunto de datos.\n",
    "\n",
    "Debemos especificar la función de pérdida a usar para evaluar un conjunto de pesos, el optimizador se usa para buscar entre diferentes pesos para la red y cualquier métrica opcional que nos gustaría recopilar. En este caso, utilizaremos la entropía cruzada como argumento de pérdida. Esta pérdida es para un problema de clasificación binaria y se define en Keras como \"binary_crossentropy\". \n",
    "\n",
    "Definiremos el optimizador como el algoritmo de descenso de gradiente estocástico eficiente “adam”. Esta es una versión popular del descenso de gradiente porque se sintoniza automáticamente y brinda buenos resultados en una amplia gama de problemas. recopilaremos e informaremos la precisión de la clasificación, definida a través del argumento de las métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar el modelo de keras\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 6: Ajustar el modelo KERAS**\n",
    "\n",
    "Hemos definido nuestro modelo y lo compilamos listo para un cálculo eficiente. Ahora vamos a ejecutar el modelo en algunos datos.\n",
    "\n",
    "El entrenamiento ocurre en epochs (épocas) y cada época se divide en lotes (batches).\n",
    "\n",
    "- Epoch: Una pasada por todas las filas del conjunto de datos de entrenamiento.\n",
    "\n",
    "- Batch: Una o más muestras consideradas por el modelo dentro de una época antes de que se actualicen los pesos.\n",
    "\n",
    "El proceso de entrenamiento se ejecutará durante un número fijo de iteraciones a través del conjunto de datos llamado epochs, que debemos especificar usando el argumento epochs. También debemos establecer la cantidad de filas del conjunto de datos que se consideran antes de que se actualicen los pesos del modelo dentro de cada epoch, lo que se denomina tamaño de batch y se establece mediante el argumento  batch_size (tamaño_lote).\n",
    "\n",
    "Para este problema, ejecutaremos una pequeña cantidad de epochs (150) y usaremos un tamaño de batch relativamente pequeño de 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 10.6695 - accuracy: 0.5234\n",
      "Epoch 2/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 2.8471 - accuracy: 0.4792\n",
      "Epoch 3/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 1.5770 - accuracy: 0.5833\n",
      "Epoch 4/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 1.2138 - accuracy: 0.6120\n",
      "Epoch 5/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 1.0117 - accuracy: 0.6341\n",
      "Epoch 6/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.9316 - accuracy: 0.6250\n",
      "Epoch 7/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.8823 - accuracy: 0.6133\n",
      "Epoch 8/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.7788 - accuracy: 0.6276\n",
      "Epoch 9/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.7285 - accuracy: 0.6628\n",
      "Epoch 10/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.7102 - accuracy: 0.6602\n",
      "Epoch 11/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6894 - accuracy: 0.6654\n",
      "Epoch 12/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6629 - accuracy: 0.6719\n",
      "Epoch 13/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6379 - accuracy: 0.6875\n",
      "Epoch 14/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6439 - accuracy: 0.6797\n",
      "Epoch 15/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6375 - accuracy: 0.6810\n",
      "Epoch 16/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6215 - accuracy: 0.6732\n",
      "Epoch 17/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6124 - accuracy: 0.6732\n",
      "Epoch 18/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6079 - accuracy: 0.6901\n",
      "Epoch 19/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6080 - accuracy: 0.6940\n",
      "Epoch 20/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6216 - accuracy: 0.6693\n",
      "Epoch 21/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5997 - accuracy: 0.6953\n",
      "Epoch 22/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6183 - accuracy: 0.6745\n",
      "Epoch 23/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5904 - accuracy: 0.7018\n",
      "Epoch 24/150\n",
      "77/77 [==============================] - 0s 1000us/step - loss: 0.5891 - accuracy: 0.7122\n",
      "Epoch 25/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5954 - accuracy: 0.6979\n",
      "Epoch 26/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5957 - accuracy: 0.7005\n",
      "Epoch 27/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5969 - accuracy: 0.6862\n",
      "Epoch 28/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5912 - accuracy: 0.7083\n",
      "Epoch 29/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5947 - accuracy: 0.6914\n",
      "Epoch 30/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5872 - accuracy: 0.6979\n",
      "Epoch 31/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5696 - accuracy: 0.7148\n",
      "Epoch 32/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5849 - accuracy: 0.7057\n",
      "Epoch 33/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5822 - accuracy: 0.7044\n",
      "Epoch 34/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5677 - accuracy: 0.7292\n",
      "Epoch 35/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5790 - accuracy: 0.7174\n",
      "Epoch 36/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5644 - accuracy: 0.7370\n",
      "Epoch 37/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5683 - accuracy: 0.7357\n",
      "Epoch 38/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5534 - accuracy: 0.7266\n",
      "Epoch 39/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5704 - accuracy: 0.7188\n",
      "Epoch 40/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5777 - accuracy: 0.7018\n",
      "Epoch 41/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5637 - accuracy: 0.7148\n",
      "Epoch 42/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5711 - accuracy: 0.7122\n",
      "Epoch 43/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5645 - accuracy: 0.7174\n",
      "Epoch 44/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5430 - accuracy: 0.7357\n",
      "Epoch 45/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5654 - accuracy: 0.7135\n",
      "Epoch 46/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5773 - accuracy: 0.7148\n",
      "Epoch 47/150\n",
      "77/77 [==============================] - 0s 999us/step - loss: 0.5402 - accuracy: 0.7318\n",
      "Epoch 48/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5501 - accuracy: 0.7227\n",
      "Epoch 49/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5680 - accuracy: 0.7188\n",
      "Epoch 50/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5444 - accuracy: 0.7409\n",
      "Epoch 51/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5596 - accuracy: 0.7266\n",
      "Epoch 52/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5690 - accuracy: 0.7240\n",
      "Epoch 53/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5686 - accuracy: 0.7253\n",
      "Epoch 54/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5399 - accuracy: 0.7500\n",
      "Epoch 55/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5571 - accuracy: 0.7083\n",
      "Epoch 56/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5607 - accuracy: 0.7135\n",
      "Epoch 57/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6115 - accuracy: 0.6914\n",
      "Epoch 58/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5427 - accuracy: 0.7292\n",
      "Epoch 59/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5412 - accuracy: 0.7331\n",
      "Epoch 60/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5464 - accuracy: 0.7357\n",
      "Epoch 61/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5510 - accuracy: 0.7253\n",
      "Epoch 62/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5470 - accuracy: 0.7279\n",
      "Epoch 63/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5394 - accuracy: 0.7461\n",
      "Epoch 64/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5532 - accuracy: 0.7370\n",
      "Epoch 65/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5593 - accuracy: 0.7188\n",
      "Epoch 66/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5617 - accuracy: 0.7227\n",
      "Epoch 67/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5374 - accuracy: 0.7539\n",
      "Epoch 68/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5295 - accuracy: 0.7448\n",
      "Epoch 69/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5477 - accuracy: 0.7292\n",
      "Epoch 70/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5298 - accuracy: 0.7305\n",
      "Epoch 71/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5277 - accuracy: 0.7435\n",
      "Epoch 72/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5288 - accuracy: 0.7500\n",
      "Epoch 73/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5487 - accuracy: 0.7331\n",
      "Epoch 74/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5351 - accuracy: 0.7396\n",
      "Epoch 75/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5451 - accuracy: 0.7227\n",
      "Epoch 76/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5359 - accuracy: 0.7331\n",
      "Epoch 77/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5287 - accuracy: 0.7331\n",
      "Epoch 78/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5301 - accuracy: 0.7552\n",
      "Epoch 79/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5349 - accuracy: 0.7409\n",
      "Epoch 80/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5318 - accuracy: 0.7487\n",
      "Epoch 81/150\n",
      "77/77 [==============================] - 0s 984us/step - loss: 0.5210 - accuracy: 0.7578\n",
      "Epoch 82/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5163 - accuracy: 0.7578\n",
      "Epoch 83/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5234 - accuracy: 0.7578\n",
      "Epoch 84/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5183 - accuracy: 0.7500\n",
      "Epoch 85/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5610 - accuracy: 0.7083\n",
      "Epoch 86/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5463 - accuracy: 0.7422\n",
      "Epoch 87/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5180 - accuracy: 0.7604\n",
      "Epoch 88/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5239 - accuracy: 0.7565\n",
      "Epoch 89/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5198 - accuracy: 0.7669\n",
      "Epoch 90/150\n",
      "77/77 [==============================] - 0s 985us/step - loss: 0.5216 - accuracy: 0.7539\n",
      "Epoch 91/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5161 - accuracy: 0.7500\n",
      "Epoch 92/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5201 - accuracy: 0.7500\n",
      "Epoch 93/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5406 - accuracy: 0.7448\n",
      "Epoch 94/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5197 - accuracy: 0.7474\n",
      "Epoch 95/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5210 - accuracy: 0.7526\n",
      "Epoch 96/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5145 - accuracy: 0.7539\n",
      "Epoch 97/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5065 - accuracy: 0.7500\n",
      "Epoch 98/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5300 - accuracy: 0.7331\n",
      "Epoch 99/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5451 - accuracy: 0.7279\n",
      "Epoch 100/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5168 - accuracy: 0.7461\n",
      "Epoch 101/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5025 - accuracy: 0.7643\n",
      "Epoch 102/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5136 - accuracy: 0.7526\n",
      "Epoch 103/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5185 - accuracy: 0.7565\n",
      "Epoch 104/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5325 - accuracy: 0.7422\n",
      "Epoch 105/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7526\n",
      "Epoch 106/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5271 - accuracy: 0.7409\n",
      "Epoch 107/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5440 - accuracy: 0.7331\n",
      "Epoch 108/150\n",
      "77/77 [==============================] - 0s 987us/step - loss: 0.5151 - accuracy: 0.7578\n",
      "Epoch 109/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5164 - accuracy: 0.7422\n",
      "Epoch 110/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5150 - accuracy: 0.7643\n",
      "Epoch 111/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5152 - accuracy: 0.7630\n",
      "Epoch 112/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5080 - accuracy: 0.7578\n",
      "Epoch 113/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5125 - accuracy: 0.7513\n",
      "Epoch 114/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5012 - accuracy: 0.7604\n",
      "Epoch 115/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5078 - accuracy: 0.7617\n",
      "Epoch 116/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5052 - accuracy: 0.7630\n",
      "Epoch 117/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5057 - accuracy: 0.7552\n",
      "Epoch 118/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5246 - accuracy: 0.7448\n",
      "Epoch 119/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5009 - accuracy: 0.7409\n",
      "Epoch 120/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5055 - accuracy: 0.7591\n",
      "Epoch 121/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5194 - accuracy: 0.7461\n",
      "Epoch 122/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5080 - accuracy: 0.7526\n",
      "Epoch 123/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5165 - accuracy: 0.7630\n",
      "Epoch 124/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4975 - accuracy: 0.7591\n",
      "Epoch 125/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5214 - accuracy: 0.7513\n",
      "Epoch 126/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5098 - accuracy: 0.7552\n",
      "Epoch 127/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5035 - accuracy: 0.7526\n",
      "Epoch 128/150\n",
      "77/77 [==============================] - 0s 994us/step - loss: 0.5033 - accuracy: 0.7617\n",
      "Epoch 129/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5023 - accuracy: 0.7591\n",
      "Epoch 130/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4933 - accuracy: 0.7826\n",
      "Epoch 131/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5183 - accuracy: 0.7422\n",
      "Epoch 132/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5425 - accuracy: 0.7344\n",
      "Epoch 133/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4949 - accuracy: 0.7708\n",
      "Epoch 134/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4916 - accuracy: 0.7721\n",
      "Epoch 135/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5014 - accuracy: 0.7565\n",
      "Epoch 136/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5028 - accuracy: 0.7630\n",
      "Epoch 137/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5124 - accuracy: 0.7565\n",
      "Epoch 138/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4944 - accuracy: 0.7591\n",
      "Epoch 139/150\n",
      "77/77 [==============================] - 0s 993us/step - loss: 0.4904 - accuracy: 0.7604\n",
      "Epoch 140/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5205 - accuracy: 0.7630\n",
      "Epoch 141/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4927 - accuracy: 0.7721\n",
      "Epoch 142/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4993 - accuracy: 0.7630\n",
      "Epoch 143/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5110 - accuracy: 0.7318\n",
      "Epoch 144/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4873 - accuracy: 0.7591\n",
      "Epoch 145/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4911 - accuracy: 0.7682\n",
      "Epoch 146/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4785 - accuracy: 0.7865\n",
      "Epoch 147/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5080 - accuracy: 0.7487\n",
      "Epoch 148/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5013 - accuracy: 0.7643\n",
      "Epoch 149/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4974 - accuracy: 0.7604\n",
      "Epoch 150/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4873 - accuracy: 0.7695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4d0071b0a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajustar el modelo de keras en el conjunto de datos\n",
    "model.fit(X, y, epochs=150, batch_size=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí es donde ocurre el trabajo en tu CPU o GPU.\n",
    "\n",
    "**Paso 7: Evaluar el modelo KERAS**\n",
    "\n",
    "Hemos entrenado nuestra red neuronal en todo el conjunto de datos y podemos evaluar el rendimiento de la red en el mismo conjunto de datos. Idealmente, puedes separar tus datos en conjuntos de entrenamiento y prueba. La función evaluar() devolverá una lista con dos valores. El primero será la pérdida del modelo en el conjunto de datos y el segundo será la precisión del modelo en el conjunto de datos. Aquí, no estamos interesados ​​en la precisión, por lo que ignoraremos el valor de la pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4707 - accuracy: 0.7786\n",
      "Accuracy: 77.86\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo de keras\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reúne todo el código anterior en un archivo .py, por ejemplo llamado 'my_first_neural_network.py'. Si intenta ejecutar este ejemplo en un cuaderno IPython o Jupyter, es posible que obtenga un error.\n",
    "\n",
    "Luego puede ejecutar el archivo python como un script desde su línea de comando de la siguiente manera:\n",
    "\n",
    "```bash\n",
    "python my_first_neural_network.py\n",
    "```\n",
    "\n",
    "Ejecutando este ejemplo, deberías ver un mensaje para cada uno de los 150 epochs imprimiendo la pérdida y la precisión, seguido por la evaluación final del modelo entrenado en el conjunto de datos de entrenamiento.\n",
    "\n",
    "Nos encantaría que la pérdida fuera a cero y la precisión a 1.0. Esto no es posible para ningún problema de aprendizaje automático, excepto los más triviales. En su lugar, siempre tendremos algún error en nuestro modelo. El objetivo es elegir una configuración de modelo y una configuración de entrenamiento que logren la pérdida más baja y la precisión más alta posible para un conjunto de datos dado.\n",
    "\n",
    "> Las redes neuronales son un algoritmo estocástico, lo que significa que el mismo algoritmo en los mismos datos puede entrenar un modelo diferente con diferentes habilidades cada vez que se ejecuta el código. Esta es una característica, no un error.\n",
    "\n",
    "> La variación en el rendimiento del modelo significa que para obtener una aproximación razonable de cómo se está desempeñando mi modelo, es posible que tenga que ajustarlo muchas veces y calcular el promedio de las puntuaciones de precisión.\n",
    "\n",
    "**Paso 8: Hacer predicciones**\n",
    "\n",
    "Entonces, después de entrenar mi modelo unas cuantas veces y obtener un promedio de todas las precisiones obtenidas, ¿cómo hago predicciones?\n",
    "\n",
    "Hacer predicciones es tan fácil como llamar a la función predict() en el modelo. Estamos usando una función de activación sigmoidal en la capa de salida, por lo que las predicciones serán una probabilidad en el rango entre 0 y 1. Podemos convertirlos fácilmente en una predicción binaria precisa para esta tarea de clasificación redondeándolos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacer predicciones de probabilidad con el modelo. En este caso, estamos usando nuevamente el mismo conjunto de datos como si fueran datos nuevos.\n",
    "predictions = model.predict(X)\n",
    "# predicciones redondas\n",
    "rounded = [round(x[0]) for x in predictions]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos convertir la probabilidad en 0 o 1 para predecir clases nítidas directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions with the model\n",
    "predictions = (model.predict(X) > 0.5).astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ejemplo completo a continuación hace predicciones para cada ejemplo en el conjunto de datos, luego imprime los datos de entrada, la clase predicha y la clase esperada para los primeros 5 ejemplos en el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primera red neuronal con keras hacer predicciones\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# cargar el conjunto de datos\n",
    "dataset = loadtxt('../assets/pima-indians-diabetes.csv', delimiter=',')\n",
    "# dividir en variables de entrada (X) y salida (y)\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "# definir el modelo de keras\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_shape=(8,), activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compilar el modelo keras\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# ajuste el modelo de keras en el conjunto de datos\n",
    "model.fit(X, y, epochs=150, batch_size=10, verbose=0)\n",
    "# hacer predicciones de clase con el modelo\n",
    "predictions = (model.predict(X) > 0.5).astype(int)\n",
    "# resumir los primeros 5 casos\n",
    "for i in range(5):\n",
    "\tprint('%s => %d (expected %d)' % (X[i].tolist(), predictions[i], y[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La razón por la que podrías obtener errores en un cuaderno de Jupyter es debido a las barras de progreso de salida durante el entrenamiento. Puedes desactivarlos fácilmente estableciendo verbose=0 en la llamada a las funciones fit() y evaluate(), como acabamos de hacer en el ejemplo.\n",
    "\n",
    "Podemos ver que la mayoría de las filas se predice correctamente. De hecho, esperaríamos que alrededor del 76.9% de las filas se predijeran correctamente de acuerdo con el rendimiento estimado del modelo en la sección anterior.\n",
    "\n",
    "**Paso 9: Guarda tu modelo**\n",
    "\n",
    "Guardar modelos requiere que tengas la biblioteca h5py instalada. Normalmente se instala como una dependencia con TensorFlow. También puedes instalarlo fácilmente de la siguiente manera:\n",
    "\n",
    "```bash\n",
    "sudo pip install h5py\n",
    "```\n",
    "\n",
    "Keras separa las preocupaciones de guardar la arquitectura de tu modelo y guardar los pesos de tu modelo.\n",
    "\n",
    "La estructura del modelo se puede describir y guardar usando dos formatos diferentes: JSON y YAML. Ambos guardan la arquitectura del modelo y los pesos por separado. Los pesos del modelo se guardan en un archivo en formato HDF5 en todos los casos.\n",
    "\n",
    "Keras también admite una interfaz más simple para guardar tanto los pesos del modelo como la arquitectura del modelo juntos en un solo archivo H5.\n",
    "\n",
    "Guardar el modelo de esta manera incluye todo lo que necesitamos saber sobre el modelo, incluyendo:\n",
    "\n",
    "Pesos del modelo.\n",
    "Arquitectura del modelo.\n",
    "Detalles de compilación del modelo (pérdida y métricas).\n",
    "Estado del optimizador del modelo.\n",
    "\n",
    "Esto significa que podemos cargar y usar el modelo directamente, sin tener que volver a compilarlo.\n",
    "\n",
    "> Nota: esta es la forma preferida para guardar y cargar tu modelo de Keras.\n",
    "\n",
    "Puedes guardar tu modelo llamando a la función save() en el modelo y especificando el nombre del archivo.\n",
    "\n",
    "El ejemplo a continuación lo demuestra primero ajustando un modelo, evaluándolo y guardándolo en el archivo model.h5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP para Pima Indians Conjunto de datos guardado en un solo archivo\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# cargar el conjunto de datos de los indios pima\n",
    "dataset = loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# dividir en variables de entrada (X) y salida (Y)\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# definir modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compilar modelo\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# ajustar el modelo\n",
    "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
    "# evaluar el modelo\n",
    "scores = model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "# guardar el modelo y la arquitectura en un solo archivo\n",
    "model.save(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un código equivalente para guardar el modelo es el siguiente:\n",
    "\n",
    "```py\n",
    "# equivalente a: modelo.guardar(\"modelo.h5\")\n",
    "from tensorflow.keras.models import save_model\n",
    "save_model(model, \"model.h5\")\n",
    "```\n",
    "\n",
    "> Si deseas saber cómo guardar su modelo usando JSON o YAML, vaya a https://machinelearningmastery.com/save-load-keras-deep-learning-models/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 10: Carga tu modelo**\n",
    "\n",
    "Tu modelo guardado puede ser cargado más tarde llamando a la función load_model() y pasando el nombre del archivo. La función devuelve el modelo con la misma arquitectura y pesos.\n",
    "\n",
    "En el siguiente código, cargamos el modelo, resumimos la arquitectura y lo evaluamos en el mismo conjunto de datos para confirmar que los pesos y la arquitectura son los mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar y evaluar un modelo guardado\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import load_model\n",
    " \n",
    "# modelo de carga\n",
    "model = load_model('model.h5')\n",
    "# resumir modelo\n",
    "model.summary()\n",
    "# cargar conjunto de datos\n",
    "dataset = loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# dividir en variables de entrada (X) y salida (Y)\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# evaluar el modelo\n",
    "score = model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuente:\n",
    "\n",
    "https://keras.io/examples/\n",
    "\n",
    "https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "\n",
    "https://keras.io/examples/vision/image_classification_from_scratch/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
