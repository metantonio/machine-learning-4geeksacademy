# Bosque Aleatorio

Ensembling es otro tipo de aprendizaje supervisado. Combina las predicciones de m√∫ltiples modelos de Machine Learning que son individualmente d√©biles para producir una predicci√≥n m√°s precisa en una nueva muestra. Al combinar modelos individuales, el modelo de conjunto tiende a ser m√°s flexibleü§∏‚Äç‚ôÄÔ∏è (menos bias) y menos sensible a los datosüßò‚Äç‚ôÄÔ∏è (menos variaci√≥n).

La idea es que los conjuntos de alumnos se desempe√±en mejor que los alumnos individuales.

En las pr√≥ximas dos lecciones, aprenderemos sobre dos t√©cnicas de conjunto, embolsado con bosques aleatorios y potenciado con XGBoost.

**¬øQu√© significa bagging (embolsar)?**

Entrenando un mont√≥n de modelos individuales de forma paralela. Cada modelo es entrenado por un subconjunto aleatorio de los datos.

**¬øC√≥mo funciona el modelo Random Forest?**

Para comprender el modelo de Random Forest, primero aprendimos sobre el √°rbol de decisiones, el componente b√°sico de un bosque aleatorio. Todos usamos √°rboles de decisi√≥n en nuestra vida diaria, e incluso si no lo sabes, reconocer√°s el proceso.

![decision_tree_daily_life](https://github.com/4GeeksAcademy/machine-learning-content/blob/master/assets/decision_tree_daily_life.jpg?raw=true)

Random Forest, como su nombre lo indica, consiste en una gran cantidad de √°rboles de decisi√≥n individuales que operan como un conjunto. Cada √°rbol individual en el Random Forest escupe una predicci√≥n de clase y la clase con m√°s votos se convierte en la predicci√≥n de nuestro modelo.

Una explicaci√≥n m√°s profunda:

A diferencia de un √°rbol de decisiones, donde cada nodo se divide en la mejor caracter√≠stica que minimiza el error, en Random Forests elegimos una selecci√≥n aleatoria de caracter√≠sticas para construir la mejor divisi√≥n. La raz√≥n de la aleatoriedad es: incluso con embolsado, cuando los √°rboles de decisi√≥n eligen la mejor caracter√≠stica para dividirse, terminan con una estructura similar y predicciones correlacionadas. Pero, empaquetar despu√©s de dividir en un subconjunto aleatorio de caracter√≠sticas significa menos correlaci√≥n entre las predicciones de los sub√°rboles.

El n√∫mero de caracter√≠sticas que se buscar√°n en cada punto de divisi√≥n se especifica como un par√°metro del algoritmo Random Forest.

Por lo tanto, en el empaquetamiento con Random Forest, cada √°rbol se construye usando una muestra aleatoria de registros y cada divisi√≥n se construye usando una muestra aleatoria de predictores.

Para aclarar la diferencia entre ellos, Random Forest es un m√©todo de conjunto que utiliza √°rboles de decisi√≥n empaquetados con subconjuntos de caracter√≠sticas aleatorias elegidos en cada punto de divisi√≥n. Luego, promedia los resultados de predicci√≥n de cada √°rbol (regresi√≥n) o usa los votos de cada √°rbol (clasificaci√≥n) para hacer la predicci√≥n final.

> La raz√≥n por la que funcionan tan bien: 'Un gran n√∫mero de modelos relativamente no correlacionados (√°rboles) que funcionan como un comit√© superar√°n a cualquiera de los modelos constituyentes individuales'. La baja correlaci√≥n es la clave.

## ¬øQu√© hiperpar√°metros se pueden ajustar para un Random Forest adem√°s de los hiperpar√°metros de cada √°rbol individual?

Siempre un buen lugar para comenzar es leer la documentaci√≥n en scikit learn: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html

Los ajustes m√°s importantes son:

- Num estimators - el n√∫mero de √°rboles de decisi√≥n en el bosque.

- Max features - n√∫mero m√°ximo de caracter√≠sticas que se eval√∫an para dividir en cada nodo.

Pero podemos intentar ajustar una amplia gama de valores en otros hiperpar√°metros como:

- max_ depth = n√∫mero m√°ximo de niveles en cada √°rbol de decisi√≥n.

- min_samples_split = n√∫mero m√≠nimo de puntos de datos colocados en un nodo antes de dividir el nodo.

- min_samples_leaf = n√∫mero m√≠nimo de puntos de datos permitidos en un nodo hoja.

- bootstrap = m√©todo para muestrear puntos de datos (con o sin reemplazo).

Veamos c√≥mo podr√≠amos implementar un RandomizedSearchCV para encontrar hiperpar√°metros √≥ptimos:

```py

from sklearn.model_selection import RandomizedSearchCV
# N√∫mero de √°rboles en Random Forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
# N√∫mero de caracter√≠sticas a considerar en cada divisi√≥n
max_features = ['auto', 'sqrt']
# N√∫mero m√°ximo de niveles en el √°rbol
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# N√∫mero m√≠nimo de muestras requeridas para dividir un nodo
min_samples_split = [2, 5, 10]
# N√∫mero m√≠nimo de muestras requeridas en cada nodo hoja
min_samples_leaf = [1, 2, 4]
# M√©todo de selecci√≥n de muestras para entrenar cada √°rbol
bootstrap = [True, False]
# Crear la cuadr√≠cula aleatoria
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
pprint(random_grid)
{'bootstrap': [True, False],
 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],
 'max_features': ['auto', 'sqrt'],
 'min_samples_leaf': [1, 2, 4],
 'min_samples_split': [2, 5, 10],
 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}
 
```

En cada iteraci√≥n, el algoritmo elegir√° diferentes combinaciones de caracter√≠sticas. ¬°En total, hay 2 * 12 * 2 * 3 * 3 * 10 = 4320 configuraciones! Sin embargo, el beneficio de una b√∫squeda aleatoria es que no estamos probando todas las combinaciones, sino que estamos seleccionando al azar para muestrear una amplia gama de valores.

## ¬øSon los modelos de Random Forest propensos al sobreajuste? ¬øPor qu√©?

No, los modelos de Random Forest generalmente no son propensos a sobreajustarse porque la selecci√≥n aleatoria de funciones y el embolsado tienden a promediar cualquier ruido en el modelo. La adici√≥n de m√°s √°rboles no provoca el sobreajuste, ya que el proceso de aleatorizaci√≥n contin√∫a promediando el ruido (m√°s √°rboles generalmente reducen el sobreajuste en el Random Forest).

En general, los algoritmos de embolsado son resistentes al sobreajuste.

Dicho esto, es posible sobreajustar con modelos de Random Forest si los √°rboles de decisi√≥n subyacentes tienen una varianza extremadamente alta. En cada punto de divisi√≥n se considera una profundidad extremadamente alta y una divisi√≥n de muestra m√≠nima baja, y un gran porcentaje de caracter√≠sticas. Por ejemplo, si todos los √°rboles son id√©nticos, el Random Forest puede sobreajustar los datos.

**¬øC√≥mo puede mi Random Forest hacer predicciones de clase precisas?**

- Necesitamos caracter√≠sticas que tengan al menos alg√∫n poder predictivo.

- Los √°rboles del bosque y sus predicciones no deben estar correlacionados (al menos correlaciones bajas). Las caracter√≠sticas y los hiperpar√°metros seleccionados afectar√°n las correlaciones finales.   

Fuente: 

https://www.dataquest.io/blog/top-10-machine-learning-algorithms-for-beginners/#:~:text=The%20first%205%20algorithms%20that,are%20examples%20of%20supervised%20learning.

https://towardsdatascience.com/understanding-random-forest-58381e0602d2

https://towardsdatascience.com/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725

https://williamkoehrsen.medium.com/random-forest-simple-explanation-377895a60d2d

https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74
