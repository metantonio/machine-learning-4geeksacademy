{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e902818",
   "metadata": {},
   "source": [
    "#  Raspado Web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e28347",
   "metadata": {},
   "source": [
    "El Raspado Web es uno de los métodos importantes para recuperar datos de un sitio web automáticamente. No todos los sitios web permiten que las personas raspen, sin embargo, puede agregar 'robots.txt' después de la URL del sitio web que deseas raspar, para saber si se le permitirá raspar o no.\n",
    "\n",
    "¿Cómo obtenemos datos de un sitio web?\n",
    "\n",
    "Hay tres formas en las que podemos obtener datos de la web:\n",
    "\n",
    "1. Importando de archivos de Internet.\n",
    "\n",
    "2. Hacer Raspado Web directamente con código para descargar el contenido HMTL.\n",
    "\n",
    "3. Consultando datos de la API del sitio web.\n",
    "\n",
    "Pero, ¿qué es una API de sitio web?\n",
    "\n",
    "Una API (Interfaz de Programación de Aplicaciones) es una interfaz de software que permite que dos aplicaciones interactúen entre sí sin la intervención del usuario. Se puede acceder a una API web a través de la web utilizando el protocolo HTTP.\n",
    "\n",
    "Las herramientas de raspado son un software especialmente desarrollado para extraer datos de sitios web. ¿Cuáles son las herramientas más comunes para el Raspado Web?\n",
    "\n",
    "- Solicitudes: Es un módulo de Python en el que podemos enviar solicitudes HTTP para recuperar contenidos. Nos ayuda a acceder a los contenidos HTML o API del sitio web mediante el envío de solicitudes Get o Post.\n",
    "\n",
    "- Beautiful Soup: Nos ayuda a analizar documentos HTML o XML en un formato legible. Podemos recuperar información más rápido.\n",
    "\n",
    "- Selenio: se utiliza principalmente para pruebas de sitios web. Ayuda a automatizar diferentes eventos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023e1ac",
   "metadata": {},
   "source": [
    "### 1. Importación de archivos planos desde la web\n",
    "\n",
    "El archivo plano que importaremos será el dataset de iris de http://archive.ics.uci.edu/ml/machine-learning-databases/iris/ obtenido del repositorio de Machine Learning UCI.\n",
    "\n",
    "Después de importarlo, lo cargaremos en un dataframe (marco de datos) de Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf2c98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5.1,3.5,1.4,0.2,Iris-setosa\n",
      "0  4.9,3.0,1.4,0.2,Iris-setosa\n",
      "1  4.7,3.2,1.3,0.2,Iris-setosa\n",
      "2  4.6,3.1,1.5,0.2,Iris-setosa\n",
      "3  5.0,3.6,1.4,0.2,Iris-setosa\n",
      "4  5.4,3.9,1.7,0.4,Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "# Importar paquete\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Importar Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Asignar url de archivo: url\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "\n",
    "# Guardar archivo localmente\n",
    "urlretrieve(url,'iris.csv')\n",
    "\n",
    "# Leer el archivo en un dataframe y mirar las primeras filas\n",
    "df = pd.read_csv('iris.csv', sep=';')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd43ce9f",
   "metadata": {},
   "source": [
    "### 2. Realización de solicitudes HTTP\n",
    "\n",
    "Pasos para hacer solicitudes HTTP:\n",
    "\n",
    "1. Inspeccionar el HTML del sitio web que queremos raspar (clic derecho).\n",
    "\n",
    "2. Acceder a la URL del sitio web usando código y descargar todo el contenido HTML en la página.\n",
    "\n",
    "3. Dar formato al contenido descargado en un formato legible.\n",
    "\n",
    "4. Extraer información útil y guardarla en un formato estructurado.\n",
    "\n",
    "5. Si la información se encuentra en varias páginas del sitio web, es posible que debamos repetir los pasos 2 a 4 para tener la información completa.\n",
    "\n",
    "**Realización de solicitudes HTTP utilizando urllib**\n",
    "\n",
    "Extraeremos el HTML en sí, pero primero empaquetaremos, enviaremos la solicitud y luego capturaremos la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77984bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar paquetes\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Especificar la URL\n",
    "url = \" https://scikit-learn.org/stable/getting_started.html\"\n",
    "\n",
    "# Esto empaqueta la solicitud\n",
    "request = Request(url)\n",
    "\n",
    "# Envíar la solicitud y capturar la respuesta\n",
    "response = urlopen(request)\n",
    "\n",
    "# Imprimir el tipo de datos de la respuesta\n",
    "print(type(response))\n",
    "\n",
    "# ¡Cierrar la respuesta!\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ad6f5",
   "metadata": {},
   "source": [
    "Esta respuesta es un objeto http.client.HTTPResponse. ¿Qué podemos hacer con él?\n",
    "\n",
    "Como viene de una página HTML, podemos leerlo para extraer el HTML usando un método read() asociado a él. Ahora extraigamos la respuesta e imprimamos el HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9610a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = Request(url)\n",
    "\n",
    "response = urlopen(request)\n",
    "\n",
    "# Extraer la respuesta: html\n",
    "html = response.read()\n",
    "\n",
    "# imprimir el html\n",
    "print(html)\n",
    "\n",
    "# ¡Cerrar la respuesta!\n",
    "response.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1401b",
   "metadata": {},
   "source": [
    "**Realizando solicitudes HTTP mediante solicitudes**\n",
    "\n",
    "Ahora vamos a usar la biblioteca de solicitudes. Esta vez no tenemos que cerrar la conexión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Especificar la url: url\n",
    "url = \"https://scikit-learn.org/stable/getting_started.html\"\n",
    "\n",
    "# Empaquetar la solicitud, envíar la solicitud y capturar la respuesta: resp\n",
    "resp = requests.get(url)\n",
    "\n",
    "# Extraer la respuesta: texto\n",
    "text = resp.text\n",
    "\n",
    "# imprimir el html\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe2056",
   "metadata": {},
   "source": [
    "**Analizando HTML usando Beautiful Soup**\n",
    "\n",
    "Aprenderemos a usar el paquete BeautifulSoup para analizar, embellecer y extraer información de HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar paquetes\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Especificar la url: url\n",
    "url = 'https://gvanrossum.github.io//'\n",
    "\n",
    "# Empaquetar la solicitud, enviar la solicitud y obtener la respuesta: resp\n",
    "resp = requests.get(url)\n",
    "\n",
    "# Extraer la respuesta como html: html_doc\n",
    "html_doc = resp.text\n",
    "\n",
    "# Luego, ¡todo lo que tenemos que hacer es convertir el documento HTML en un objeto BeautifulSoup!\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Embellecer el objeto BeautifulSoup: pretty_soup\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# Imprimir la respuesta\n",
    "print(pretty_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa08e2",
   "metadata": {},
   "source": [
    "**Los tags se pueden llamar de diferentes maneras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2810657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta línea de código crea un objeto BeautifulSoup desde una página web:\n",
    " \n",
    "soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    " \n",
    "# Dentro del objeto `sopa`, los tags se pueden llamar por su nombre:\n",
    " \n",
    "first_div = soup.div\n",
    " \n",
    "# O por selector de CSS:\n",
    " \n",
    "all_elements_of_header_class = soup.select(\".header\")\n",
    " \n",
    "# O por una llamada a `.find_all`:\n",
    " \n",
    "all_p_elements = soup.find_all(\"p\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67fd5f1b",
   "metadata": {},
   "source": [
    "### 3. Interactuando con APIs\n",
    "\n",
    "Es un poco más complicado que raspar el documento HTML, especialmente si se requiere autenticación, pero los datos serán más estructurados y estables.\n",
    "\n",
    "Pasos para consultar datos de la API del sitio web:\n",
    "\n",
    "1. Inspeccionar la sección de red XHR de la URL que queremos raspar.\n",
    "\n",
    "2. Averiguar la petición-respuesta que nos da los datos que queremos.\n",
    "\n",
    "3. Dependiendo del tipo de solicitud (publicar u obtener), simulemos la solicitud en nuestro código y recuperemos los datos de la API. Si se requiere autenticación, primero deberemos solicitar el token antes de enviar nuestra solicitud POST.\n",
    "\n",
    "4. Extraer información útil que necesitamos.\n",
    "\n",
    "5. Para API con un límite en el tamaño de la consulta, necesitaremos usar 'for loop' para recuperar repetidamente todos los datos\n",
    "\n",
    "**Ejemplo: cargando y explorando un Json con solicitud GET**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce3a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar paquete\n",
    "import requests\n",
    "\n",
    "# Asignar url a la variable: url\n",
    "url = \"https://covid-19-statistics.p.rapidapi.com/regions\"\n",
    "\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Host\": \"covid-19-statistics.p.rapidapi.com\",\n",
    "\t\"X-RapidAPI-Key\": \"SIGN-UP-FOR-KEY\"\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "\n",
    "# Decodificar los datos JSON en un diccionario: json_data\n",
    "json_data = response.json()\n",
    "\n",
    "# Imprimir cada par clave-valor en json_data\n",
    "for k in json_data.keys():\n",
    "    print(k + ': ', json_data[k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a0b233",
   "metadata": {},
   "source": [
    "Si deseas raspar un sitio web, primero debemos verificar la existencia de API en la sección de red usando inspeccionar. Si podemos encontrar la respuesta a una solicitud que nos proporcione todos los datos que necesitamos, podemos construir una solución estable. Si no podemos encontrar los datos en la red, podemos intentar usar solicitudes o Selenium para descargar contenido HTML y usar Beautiful Soup para formatear los datos.\n",
    "\n",
    "Otras herramientas principales de Raspado Web en 2022:\n",
    "\n",
    "1. Newsdata.io\n",
    "\n",
    "2. Scrapingbee \n",
    "\n",
    "3. Bright Data\n",
    "\n",
    "4. Scraping-bot \n",
    "\n",
    "5. Scraper API \n",
    "\n",
    "6. Scrapestack \n",
    "\n",
    "7. Apify \n",
    "\n",
    "8. Agenty \n",
    "\n",
    "9. Import.io\n",
    "\n",
    "10. Outwit \n",
    "\n",
    "11. Webz.io "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1945b8a8",
   "metadata": {},
   "source": [
    "Referencias: \n",
    "\n",
    "https://towardsdatascience.com/web-scraping-basics-82f8b5acd45c\n",
    "\n",
    "https://rapidapi.com/rapidapi/api\n",
    "\n",
    "https://newsdata.io/blog/top-21-web-scraping-tools-for-you/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
